# STEP 1.
## First of all installation of speckit plus & claud code (if u cant afford use gemini free setup for claude code).
# STEP 2.
## NOW CREATE A FOLDER HACKATHON 1 / ANY NAME  AND OPEN IT WITH POWERSHELL.
# STEP 3.
## NOW INITIALIZE SPECKIT PLUS PROJECT WITH THIS CMD(specifyplus init name of project).
# STEP 4.
## NOW OPEN IT ON VS CODE (code .) AND CHECK WHICH FILES ARE CREATED.
# STEP 5.
## NOW START CLAUDE CODE IN OLD USING POWERSHELL WITH CMD (ccr code or claud )
# STEP 6.
## NOW CREATE A SPEC DRIVEN WAY STEP FOR CREATE BOOK LIKE /sp.constitution (ready this file by chatgpt by giving them better prompt like
write constitution as per my project requirments.
i am using speckitplus for writing book project 1 

1. AI/Spec-Driven Book Creation: Write a book using Docusaurus and deploy it to GitHub Pages. You will use Spec-Kit Plus ( https://github.com/panaversity/spec-kit-plus/ ) and Claude Code ( https://www.claude.com/product/claude-code ) to write the book. 
as per speckit recomandation first i need to write constitutions here is constitution prompt reference in their book:
/sp.constitution

Project: Research paper on AI-native software development

Core principles:
- Accuracy through primary source verification
- Clarity for academic audience (computer science background)
- Reproducibility (all claims cited and traceable)
- Rigor (peer-reviewed sources preferred)

Key standards:
- All factual claims must be traceable to sources
- Citation format: APA style
- Source types: minimum 50% peer-reviewed articles
- Plagiarism check: 0% tolerance before submission
- Writing clarity: Flesch-Kincaid grade 10-12

Constraints:
- Word count: 5,000-7,000 words
- Minimum 15 sources
- Format: PDF with embedded citations

Success criteria:
- All claims verified against sources
- Zero plagiarism detected
- Passes fact-checking review
remember make it copy paste so i can easliy copy it
 # STEP 7.
 ## CHATGPT WILL CREATE PROMPT LIKE THIS COPY IT AND PASTE IT ON CLAUDE CODE.
 ```
 /sp.constitution

Project: AI/Spec-Driven Book Creation Using Docusaurus, GitHub Pages, Spec-Kit Plus, and Claude Code

Core principles:
- Accuracy through primary source verification
- Clarity for a technical/developer audience
- Reproducibility (all instructions, code, and workflows must be cited and traceable)
- Rigor (official documentation, technical standards, or peer-reviewed sources preferred)

Key standards:
- All factual claims must be traceable to credible sources
- Citation format: APA style
- Source types: minimum 50% high-credibility or peer-reviewed/official documentation sources
- Plagiarism check: 0% tolerance before submission
- Writing clarity: Flesch-Kincaid grade 10‚Äì12

Constraints:
- Word count: 5,000‚Äì7,000 words
- Minimum 15 sources
- Format: Docusaurus site + PDF with embedded citations
- Tools: Spec-Kit Plus, Claude Code, Docusaurus, GitHub Pages

Success criteria:
- All claims verified against credible sources
- Zero plagiarism detected
- Passes fact-checking review
- Docusaurus site builds without errors
- GitHub Pages deployment successful and accessible
- PDF export includes proper APA-style embedded citations
```

# STEP 8.
## NOW CREATE A SPEC DRIVEN WAY STEP FOR CREATE BOOK LIKE /sp.specify (ready this file by chatgpt by giving them better prompt like
```
u have to create a /sp.specify file for my spec driven book here is a sample of specify file 

/sp.specify Research paper on AI's impact on K-12 classroom efficiency

Target audience: Education administrators evaluating AI adoption
Focus: Teacher workload reduction and student outcome improvements

Success criteria:
- Identifies 3+ concrete AI applications with evidence
- Cites 8+ peer-reviewed academic sources
- Reader can explain ROI of classroom AI after reading
- All claims supported by evidence

Constraints:
- Word count: 3000-5000 words
- Format: Markdown source, APA citations
- Sources: Peer-reviewed journals, published within past 10 years
- Timeline: Complete within 2 weeks

Not building:
- Comprehensive literature review of entire AI field
- Comparison of specific AI products/vendors
- Discussion of ethical concerns (separate paper)
- Implementation guide or code examples

# and here is my heackathon requirements:
Hackathon I: Create a Textbook for Teaching Physical AI & Humanoid Robotics Course
The future of work will be a partnership between people, intelligent agents (AI software), and robots. This shift won't necessarily eliminate jobs but will change what humans do, leading to a massive demand for new skills. We have already written a book on AI agents. Therefore, we want you to write a textbook to teach a course in Physical AI & Humanoid Robotics (The course details are documented below). 

Excel in the Hackathon and Launch Your Journey as an AI Startup Founder üöÄ
We‚Äôve recently launched Panaversity (panaversity.org), an initiative focused on teaching cutting-edge AI courses. Alongside this, we‚Äôre working on publishing our first book, which you can explore at ai-native.panaversity.org. Our next milestone is to build a portal where authors can create AI-native technical textbooks, and readers can easily access and learn from them using AI Agents. We also plan to publish O/A Level, Science, Engineering, and Medical AI-native books to support students and professionals across disciplines. If you perform well in this hackathon, you may be invited for an interview to join the Panaversity core team and potentially step into the role of a startup founder within this growing ecosystem. You will get a chance to work with Panaversity founders Zia, Rehan, Junaid, and Wania and become the very best. You may also get a chance to teach at Panaversity, PIAIC, and GIAIC. 
Requirements

You are required to complete a unified book project using Claude Code and Spec-Kit Plus. The core deliverables are:

1. AI/Spec-Driven Book Creation: Write a book using Docusaurus and deploy it to GitHub Pages. You will use Spec-Kit Plus ( https://github.com/panaversity/spec-kit-plus/ ) and Claude Code ( https://www.claude.com/product/claude-code ) to write the book. 

2. Integrated RAG Chatbot Development: Build and embed a Retrieval-Augmented Generation (RAG) chatbot within the published book. This chatbot, utilizing the OpenAI Agents/ChatKit SDKs, FastAPI, Neon Serverless Postgres database, and Qdrant Cloud Free Tier, must be able to answer user questions about the book's content, including answering questions based only on text selected by the user.

3. Participants will receive points out of 100, for base functionality defined above. 

4. Participants can earn up to 50 extra bonus points by creating and using reusable intelligence via Claude Code Subagents and Agent Skills in the book project.

5. Participants can receive up to 50 extra bonus points if they also implement Signup and Signin using https://www.better-auth.com/ At signup you will ask questions from the user about their software and hardware background. Knowing the background of the user we will be able to personalize the content.

6.  Participants can receive up to 50 extra bonus points if the logged user can personalise the content in the chapters by pressing a button at the start of each chapter. 

7. Participants can receive up to 50 extra bonus points if the logged user can translate the content in Urdu in the chapters by pressing a button at the start of each chapter. 

Timeline
Submission Deadline: Sunday, Nov 30, 2025 at 06:00 PM (form will close)
Live Presentations: Sunday, Nov 30, 2025 starting at 6:00 PM on Zoom

Top submissions will be invited via WhatsApp to present live on Zoom. 
Note: All submissions will be evaluated. Live presentation is by invitation only, but does not affect final scoring. 
Submit and Present Your Project:

Once you have completed the project you will submit your project here:

https://forms.gle/CQsSEGM3GeCrL43c8 

Submit the following via the form:
Public GitHub Repo Link
Published Book Link for Github Pages or Vercel. 
Include a demo video link (must be under 90 seconds). Judges will only watch the first 90 seconds. You can use NotebookLM or record your demo.
WhatsApp number (top submissions will be invited to present live)
Everyone is welcome to join the Zoom meeting to watch the presentations. Only invited participants will present their submissions. Meeting starts at 6:00 PM on Sunday, Nov 30:

Join Zoom Meeting
Time: Nov 30, 2025 06:00 PM
https://us06web.zoom.us/j/84976847088?pwd=Z7t7NaeXwVmmR5fysCv7NiMbfbhIda.1 
Meeting ID: 849 7684 7088
Passcode: 305850


The Course Details
Physical AI & Humanoid Robotics
Focus and Theme: AI Systems in the Physical World. Embodied Intelligence.
Goal: Bridging the gap between the digital brain and the physical body. Students apply their AI knowledge to control Humanoid Robots in simulated and real-world environments.
Quarter Overview
The future of AI extends beyond digital spaces into the physical world. This capstone quarter introduces Physical AI‚ÄîAI systems that function in reality and comprehend physical laws. Students learn to design, simulate, and deploy humanoid robots capable of natural human interactions using ROS 2, Gazebo, and NVIDIA Isaac.
Module 1: The Robotic Nervous System (ROS 2)
Focus: Middleware for robot control.
ROS 2 Nodes, Topics, and Services.
Bridging Python Agents to ROS controllers using rclpy.
Understanding URDF (Unified Robot Description Format) for humanoids.


Module 2: The Digital Twin (Gazebo & Unity)
Focus: Physics simulation and environment building.
Simulating physics, gravity, and collisions in Gazebo.
High-fidelity rendering and human-robot interaction in Unity.
Simulating sensors: LiDAR, Depth Cameras, and IMUs.


Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
Focus: Advanced perception and training.
NVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation.
Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation.
Nav2: Path planning for bipedal humanoid movement.


Module 4: Vision-Language-Action (VLA)
Focus: The convergence of LLMs and Robotics.	
Voice-to-Action: Using OpenAI Whisper for voice commands.
Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.
Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.

Why Physical AI Matters
Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.
Learning Outcomes
Understand Physical AI principles and embodied intelligence
Master ROS 2 (Robot Operating System) for robotic control
Simulate robots with Gazebo and Unity
Develop with NVIDIA Isaac AI robot platform
Design humanoid robots for natural interactions
Integrate GPT models for conversational robotics
Weekly Breakdown
Weeks 1-2: Introduction to Physical AI
Foundations of Physical AI and embodied intelligence
From digital AI to robots that understand physical laws
Overview of humanoid robotics landscape
Sensor systems: LIDAR, cameras, IMUs, force/torque sensors
Weeks 3-5: ROS 2 Fundamentals
ROS 2 architecture and core concepts
Nodes, topics, services, and actions
Building ROS 2 packages with Python
Launch files and parameter management
Weeks 6-7: Robot Simulation with Gazebo
Gazebo simulation environment setup
URDF and SDF robot description formats
Physics simulation and sensor simulation
Introduction to Unity for robot visualization
Weeks 8-10: NVIDIA Isaac Platform
NVIDIA Isaac SDK and Isaac Sim
AI-powered perception and manipulation
Reinforcement learning for robot control
Sim-to-real transfer techniques
Weeks 11-12: Humanoid Robot Development
Humanoid robot kinematics and dynamics
Bipedal locomotion and balance control
Manipulation and grasping with humanoid hands
Natural human-robot interaction design

Week 13: Conversational Robotics
Integrating GPT models for conversational AI in robots
Speech recognition and natural language understanding
Multi-modal interaction: speech, gesture, vision
Assessments
ROS 2 package development project
Gazebo simulation implementation
Isaac-based perception pipeline
Capstone: Simulated humanoid robot with conversational AI

Hardware Requirements
This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).

Because the capstone involves a "Simulated Humanoid," the primary investment must be in High-Performance Workstations. However, to fulfill the "Physical AI" promise, you also need Edge Computing Kits (brains without bodies) or specific robot hardware.

1. The "Digital Twin" Workstation (Required per Student)
This is the most critical component. NVIDIA Isaac Sim is an Omniverse application that requires "RTX" (Ray Tracing) capabilities. Standard laptops (MacBooks or non-RTX Windows machines) will not work.
GPU (The Bottleneck): NVIDIA RTX 4070 Ti (12GB VRAM) or higher.
Why: You need high VRAM to load the USD (Universal Scene Description) assets for the robot and environment, plus run the VLA (Vision-Language-Action) models simultaneously.
Ideal: RTX 3090 or 4090 (24GB VRAM) allows for smoother "Sim-to-Real" training.
CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9.
Why: Physics calculations (Rigid Body Dynamics) in Gazebo/Isaac are CPU-intensive.
RAM: 64 GB DDR5 (32 GB is the absolute minimum, but will crash during complex scene rendering).
OS: Ubuntu 22.04 LTS.
Note: While Isaac Sim runs on Windows, ROS 2 (Humble/Iron) is native to Linux. Dual-booting or dedicated Linux machines are mandatory for a friction-free experience.


2. The "Physical AI" Edge Kit
Since a full humanoid robot is expensive, students learn "Physical AI" by setting up the nervous system on a desk before deploying it to a robot. This kit covers Module 3 (Isaac ROS) and Module 4 (VLA).
The Brain: NVIDIA Jetson Orin Nano (8GB) or Orin NX (16GB).
Role: This is the industry standard for embodied AI. Students will deploy their ROS 2 nodes here to understand resource constraints vs. their powerful workstations.
The Eyes (Vision): Intel RealSense D435i or D455.
Role: Provides RGB (Color) and Depth (Distance) data. Essential for the VSLAM and Perception modules.
The Inner Ear (Balance): Generic USB IMU (BNO055) (Often built into the RealSense D435i or Jetson boards, but a separate module helps teach IMU calibration).
Voice Interface: A simple USB Microphone/Speaker array (e.g., ReSpeaker) for the "Voice-to-Action" Whisper integration.
3. The Robot Lab
For the "Physical" part of the course, you have three tiers of options depending on budget.
Option A: The "Proxy" Approach (Recommended for Budget)
Use a quadruped (dog) or a robotic arm as a proxy. The software principles (ROS 2, VSLAM, Isaac Sim) transfer 90% effectively to humanoids.
Robot: Unitree Go2 Edu (~$1,800 - $3,000).
Pros: Highly durable, excellent ROS 2 support, affordable enough to have multiple units.
Cons: Not a biped (humanoid).
Option B: The "Miniature Humanoid" Approach
Small, table-top humanoids.
Robot: Unitree H1 is too expensive ($90k+), so look at Unitree G1 (~$16k) or Robotis OP3 (older, but stable, ~$12k).
Budget Alternative: Hiwonder TonyPi Pro (~$600).
Warning: The cheap kits (Hiwonder) usually run on Raspberry Pi, which cannot run NVIDIA Isaac ROS efficiently. You would use these only for kinematics (walking) and use the Jetson kits for AI.
Option C: The "Premium" Lab (Sim-to-Real specific)
If the goal is to actually deploy the Capstone to a real humanoid:
Robot: Unitree G1 Humanoid.
Why: It is one of the few commercially available humanoids that can actually walk dynamically and has an SDK open enough for students to inject their own ROS 2 controllers.

4. Summary of Architecture
To teach this successfully, your lab infrastructure should look like this:
Component
Hardware
Function
Sim Rig
PC with RTX 4080 + Ubuntu 22.04
Runs Isaac Sim, Gazebo, Unity, and trains LLM/VLA models.
Edge Brain
Jetson Orin Nano
Runs the "Inference" stack. Students deploy their code here.
Sensors
RealSense Camera + Lidar
Connected to the Jetson to feed real-world data to the AI.
Actuator
Unitree Go2 or G1 (Shared)
Receives motor commands from the Jetson.


If you do not have access to RTX-enabled workstations, we must restructure the course to rely entirely on cloud-based instances (like AWS RoboMaker or NVIDIA's cloud delivery for Omniverse), though this introduces significant latency and cost complexity.

Building a "Physical AI" lab is a significant investment. You will have to choose between building a physical On-Premise Lab at Home (High CapEx) versus running a Cloud-Native Lab (High OpEx).

Option 2 High OpEx: The "Ether" Lab (Cloud-Native)
Best for: Rapid deployment, or students with weak laptops.
1. Cloud Workstations (AWS/Azure) Instead of buying PCs, you rent instances.
Instance Type: AWS g5.2xlarge (A10G GPU, 24GB VRAM) or g6e.xlarge.
Software: NVIDIA Isaac Sim on Omniverse Cloud (requires specific AMI).
Cost Calculation:
Instance cost: ~$1.50/hour (spot/on-demand mix).
Usage: 10 hours/week √ó 12 weeks = 120 hours.
Storage (EBS volumes for saving environments): ~$25/quarter.
Total Cloud Bill: ~$205 per quarter.
2. Local "Bridge" Hardware You cannot eliminate hardware entirely for "Physical AI." You still need the edge devices to deploy the code physically.
Edge AI Kits: You still need the Jetson Kit for the physical deployment phase.
Cost: $700 (One-time purchase).
Robot: You still need one physical robot for the final demo.
Cost: $3,000 (Unitree Go2 Standard).
The Economy Jetson Student Kit
Best for: Learning ROS 2, Basic Computer Vision, and Sim-to-Real control.
Component
Model
Price (Approx.)
Notes
The Brain
NVIDIA Jetson Orin Nano Super Dev Kit (8GB)
$249
New official MSRP (Price dropped from ~$499). Capable of 40 TOPS.
The Eyes
Intel RealSense D435i
$349
Includes IMU (essential for SLAM). Do not buy the D435 (non-i).
The Ears
ReSpeaker USB Mic Array v2.0
$69
Far-field microphone for voice commands (Module 4).
Wi-Fi
(Included in Dev Kit)
$0
The new "Super" kit includes the Wi-Fi module pre-installed.
Power/Misc
SD Card (128GB) + Jumper Wires
$30
High-endurance microSD card required for the OS.
TOTAL


~$700 per kit




3. The Latency Trap (Hidden Cost)
Simulating in the cloud works well, but controlling a real robot from a cloud instance is dangerous due to latency.
Solution: Students train in the Cloud, download the model (weights), and flash it to the local Jetson kit.
```

 # STEP 9.
 ## CHATGPT WILL CREATE PROMPT LIKE THIS COPY IT AND PASTE IT ON CLAUDE CODE.
 ```
 /sp.specify Physical AI & Humanoid Robotics Textbook Project
Target audience:

- Students learning robotics and AI

- Engineering and CS learners in Panaversity/GIAIC/PIAIC

- Beginners-to-intermediate developers entering Physical AI and humanoid robotics



Focus:

- Create a full textbook teaching Physical AI & Humanoid Robotics

- Use Spec-Kit Plus + Claude Code to generate all chapters

- Build and embed a working RAG chatbot into the published book

- Include hands-on tutorials using ROS 2, Gazebo, Unity, Isaac Sim, and VLA robotics

- Deploy the final book using Docusaurus on GitHub Pages/Vercel



Success criteria:

- 12‚Äì15 high-quality chapters covering:

  - Physical AI foundations

  - ROS 2 basics and advanced workflows

  - Gazebo & Unity simulation

  - NVIDIA Isaac Sim + Isaac ROS

  - Humanoid robot kinematics/dynamics

  - Vision-Language-Action robotics

  - Conversational robotics with GPT models

- Each chapter generated using Spec-Kit Plus

- A working RAG chatbot:

  - Uses FastAPI backend

  - Qdrant Cloud for vector storage

  - Neon Postgres for metadata

  - Uses OpenAI Agents/ChatKit SDK

  - Can answer from entire book or from selected text

- Deployed public book link + GitHub repo link

- 90-second demo video included

- Submission before Nov 30, 2025, 6 PM



Bonus (optional):

- +50 pts: Reusable intelligence with Claude Code Subagents and Agent Skills

- +50 pts: Signup/Signin using better-auth.com with background questions

- +50 pts: Personalize chapter content using a ‚ÄúPersonalize‚Äù button

- +50 pts: Translate chapters to Urdu using a ‚ÄúTranslate to Urdu‚Äù button



Constraints:

- Format: Markdown/MDX for Docusaurus

- Must use Spec-Kit Plus workflow

- Chatbot answers only from retrieved text

- Tech stack required:

  - FastAPI

  - Qdrant Cloud (Free Tier)

  - Neon Serverless Postgres

  - OpenAI Agents/ChatKit SDK

- Content must match course modules and weekly outline

- No video course, no full humanoid firmware, no out-of-scope robotics research



Not building:

- Full robotics MOOC or extended video lectures

- SLAM research papers

- Robot marketplace or e-commerce features

- Mobile app

- Cloud-only Isaac Sim course

- Robot firmware for real humanoids



Deliverables checklist:

- 12‚Äì15 complete chapters

- Capstone: Autonomous Humanoid system

- Hands-on code for ROS 2, Gazebo, Isaac

- Hardware setup guides (Jetson, RealSense, Go2/G1)

- Embedded RAG chatbot inside Docusaurus site

- Personalization + Urdu translation buttons (if doing bonuses)

- Public GitHub repo + published book link

- 90-second demo video
```

# STEP 10.
## NOW CREATE A SPEC DRIVEN WAY STEP FOR CREATE BOOK LIKE /sp.plan (ready this file by chatgpt by giving them better prompt like:)
```
as u know my hackathon requirement here .
Hackathon I: Create a Textbook for Teaching Physical AI & Humanoid Robotics Course
The future of work will be a partnership between people, intelligent agents (AI software), and robots. This shift won't necessarily eliminate jobs but will change what humans do, leading to a massive demand for new skills. We have already written a book on AI agents. Therefore, we want you to write a textbook to teach a course in Physical AI & Humanoid Robotics (The course details are documented below). 

Excel in the Hackathon and Launch Your Journey as an AI Startup Founder üöÄ
We‚Äôve recently launched Panaversity (panaversity.org), an initiative focused on teaching cutting-edge AI courses. Alongside this, we‚Äôre working on publishing our first book, which you can explore at ai-native.panaversity.org. Our next milestone is to build a portal where authors can create AI-native technical textbooks, and readers can easily access and learn from them using AI Agents. We also plan to publish O/A Level, Science, Engineering, and Medical AI-native books to support students and professionals across disciplines. If you perform well in this hackathon, you may be invited for an interview to join the Panaversity core team and potentially step into the role of a startup founder within this growing ecosystem. You will get a chance to work with Panaversity founders Zia, Rehan, Junaid, and Wania and become the very best. You may also get a chance to teach at Panaversity, PIAIC, and GIAIC. 
Requirements

You are required to complete a unified book project using Claude Code and Spec-Kit Plus. The core deliverables are:

1. AI/Spec-Driven Book Creation: Write a book using Docusaurus and deploy it to GitHub Pages. You will use Spec-Kit Plus ( https://github.com/panaversity/spec-kit-plus/ ) and Claude Code ( https://www.claude.com/product/claude-code ) to write the book. 

2. Integrated RAG Chatbot Development: Build and embed a Retrieval-Augmented Generation (RAG) chatbot within the published book. This chatbot, utilizing the OpenAI Agents/ChatKit SDKs, FastAPI, Neon Serverless Postgres database, and Qdrant Cloud Free Tier, must be able to answer user questions about the book's content, including answering questions based only on text selected by the user.

3. Participants will receive points out of 100, for base functionality defined above. 

4. Participants can earn up to 50 extra bonus points by creating and using reusable intelligence via Claude Code Subagents and Agent Skills in the book project.

5. Participants can receive up to 50 extra bonus points if they also implement Signup and Signin using https://www.better-auth.com/ At signup you will ask questions from the user about their software and hardware background. Knowing the background of the user we will be able to personalize the content.

6.  Participants can receive up to 50 extra bonus points if the logged user can personalise the content in the chapters by pressing a button at the start of each chapter. 

7. Participants can receive up to 50 extra bonus points if the logged user can translate the content in Urdu in the chapters by pressing a button at the start of each chapter. 

Timeline
Submission Deadline: Sunday, Nov 30, 2025 at 06:00 PM (form will close)
Live Presentations: Sunday, Nov 30, 2025 starting at 6:00 PM on Zoom

Top submissions will be invited via WhatsApp to present live on Zoom. 
Note: All submissions will be evaluated. Live presentation is by invitation only, but does not affect final scoring. 
Submit and Present Your Project:

Once you have completed the project you will submit your project here:

https://forms.gle/CQsSEGM3GeCrL43c8 

Submit the following via the form:
Public GitHub Repo Link
Published Book Link for Github Pages or Vercel. 
Include a demo video link (must be under 90 seconds). Judges will only watch the first 90 seconds. You can use NotebookLM or record your demo.
WhatsApp number (top submissions will be invited to present live)
Everyone is welcome to join the Zoom meeting to watch the presentations. Only invited participants will present their submissions. Meeting starts at 6:00 PM on Sunday, Nov 30:

Join Zoom Meeting
Time: Nov 30, 2025 06:00 PM
https://us06web.zoom.us/j/84976847088?pwd=Z7t7NaeXwVmmR5fysCv7NiMbfbhIda.1 
Meeting ID: 849 7684 7088
Passcode: 305850


The Course Details
Physical AI & Humanoid Robotics
Focus and Theme: AI Systems in the Physical World. Embodied Intelligence.
Goal: Bridging the gap between the digital brain and the physical body. Students apply their AI knowledge to control Humanoid Robots in simulated and real-world environments.
Quarter Overview
The future of AI extends beyond digital spaces into the physical world. This capstone quarter introduces Physical AI‚ÄîAI systems that function in reality and comprehend physical laws. Students learn to design, simulate, and deploy humanoid robots capable of natural human interactions using ROS 2, Gazebo, and NVIDIA Isaac.
Module 1: The Robotic Nervous System (ROS 2)
Focus: Middleware for robot control.
ROS 2 Nodes, Topics, and Services.
Bridging Python Agents to ROS controllers using rclpy.
Understanding URDF (Unified Robot Description Format) for humanoids.


Module 2: The Digital Twin (Gazebo & Unity)
Focus: Physics simulation and environment building.
Simulating physics, gravity, and collisions in Gazebo.
High-fidelity rendering and human-robot interaction in Unity.
Simulating sensors: LiDAR, Depth Cameras, and IMUs.


Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)
Focus: Advanced perception and training.
NVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation.
Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation.
Nav2: Path planning for bipedal humanoid movement.


Module 4: Vision-Language-Action (VLA)
Focus: The convergence of LLMs and Robotics.	
Voice-to-Action: Using OpenAI Whisper for voice commands.
Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.
Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.

Why Physical AI Matters
Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.
Learning Outcomes
Understand Physical AI principles and embodied intelligence
Master ROS 2 (Robot Operating System) for robotic control
Simulate robots with Gazebo and Unity
Develop with NVIDIA Isaac AI robot platform
Design humanoid robots for natural interactions
Integrate GPT models for conversational robotics
Weekly Breakdown
Weeks 1-2: Introduction to Physical AI
Foundations of Physical AI and embodied intelligence
From digital AI to robots that understand physical laws
Overview of humanoid robotics landscape
Sensor systems: LIDAR, cameras, IMUs, force/torque sensors
Weeks 3-5: ROS 2 Fundamentals
ROS 2 architecture and core concepts
Nodes, topics, services, and actions
Building ROS 2 packages with Python
Launch files and parameter management
Weeks 6-7: Robot Simulation with Gazebo
Gazebo simulation environment setup
URDF and SDF robot description formats
Physics simulation and sensor simulation
Introduction to Unity for robot visualization
Weeks 8-10: NVIDIA Isaac Platform
NVIDIA Isaac SDK and Isaac Sim
AI-powered perception and manipulation
Reinforcement learning for robot control
Sim-to-real transfer techniques
Weeks 11-12: Humanoid Robot Development
Humanoid robot kinematics and dynamics
Bipedal locomotion and balance control
Manipulation and grasping with humanoid hands
Natural human-robot interaction design

Week 13: Conversational Robotics
Integrating GPT models for conversational AI in robots
Speech recognition and natural language understanding
Multi-modal interaction: speech, gesture, vision
Assessments
ROS 2 package development project
Gazebo simulation implementation
Isaac-based perception pipeline
Capstone: Simulated humanoid robot with conversational AI

Hardware Requirements
This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).

Because the capstone involves a "Simulated Humanoid," the primary investment must be in High-Performance Workstations. However, to fulfill the "Physical AI" promise, you also need Edge Computing Kits (brains without bodies) or specific robot hardware.

1. The "Digital Twin" Workstation (Required per Student)
This is the most critical component. NVIDIA Isaac Sim is an Omniverse application that requires "RTX" (Ray Tracing) capabilities. Standard laptops (MacBooks or non-RTX Windows machines) will not work.
GPU (The Bottleneck): NVIDIA RTX 4070 Ti (12GB VRAM) or higher.
Why: You need high VRAM to load the USD (Universal Scene Description) assets for the robot and environment, plus run the VLA (Vision-Language-Action) models simultaneously.
Ideal: RTX 3090 or 4090 (24GB VRAM) allows for smoother "Sim-to-Real" training.
CPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9.
Why: Physics calculations (Rigid Body Dynamics) in Gazebo/Isaac are CPU-intensive.
RAM: 64 GB DDR5 (32 GB is the absolute minimum, but will crash during complex scene rendering).
OS: Ubuntu 22.04 LTS.
Note: While Isaac Sim runs on Windows, ROS 2 (Humble/Iron) is native to Linux. Dual-booting or dedicated Linux machines are mandatory for a friction-free experience.


2. The "Physical AI" Edge Kit
Since a full humanoid robot is expensive, students learn "Physical AI" by setting up the nervous system on a desk before deploying it to a robot. This kit covers Module 3 (Isaac ROS) and Module 4 (VLA).
The Brain: NVIDIA Jetson Orin Nano (8GB) or Orin NX (16GB).
Role: This is the industry standard for embodied AI. Students will deploy their ROS 2 nodes here to understand resource constraints vs. their powerful workstations.
The Eyes (Vision): Intel RealSense D435i or D455.
Role: Provides RGB (Color) and Depth (Distance) data. Essential for the VSLAM and Perception modules.
The Inner Ear (Balance): Generic USB IMU (BNO055) (Often built into the RealSense D435i or Jetson boards, but a separate module helps teach IMU calibration).
Voice Interface: A simple USB Microphone/Speaker array (e.g., ReSpeaker) for the "Voice-to-Action" Whisper integration.
3. The Robot Lab
For the "Physical" part of the course, you have three tiers of options depending on budget.
Option A: The "Proxy" Approach (Recommended for Budget)
Use a quadruped (dog) or a robotic arm as a proxy. The software principles (ROS 2, VSLAM, Isaac Sim) transfer 90% effectively to humanoids.
Robot: Unitree Go2 Edu (~$1,800 - $3,000).
Pros: Highly durable, excellent ROS 2 support, affordable enough to have multiple units.
Cons: Not a biped (humanoid).
Option B: The "Miniature Humanoid" Approach
Small, table-top humanoids.
Robot: Unitree H1 is too expensive ($90k+), so look at Unitree G1 (~$16k) or Robotis OP3 (older, but stable, ~$12k).
Budget Alternative: Hiwonder TonyPi Pro (~$600).
Warning: The cheap kits (Hiwonder) usually run on Raspberry Pi, which cannot run NVIDIA Isaac ROS efficiently. You would use these only for kinematics (walking) and use the Jetson kits for AI.
Option C: The "Premium" Lab (Sim-to-Real specific)
If the goal is to actually deploy the Capstone to a real humanoid:
Robot: Unitree G1 Humanoid.
Why: It is one of the few commercially available humanoids that can actually walk dynamically and has an SDK open enough for students to inject their own ROS 2 controllers.

4. Summary of Architecture
To teach this successfully, your lab infrastructure should look like this:
Component
Hardware
Function
Sim Rig
PC with RTX 4080 + Ubuntu 22.04
Runs Isaac Sim, Gazebo, Unity, and trains LLM/VLA models.
Edge Brain
Jetson Orin Nano
Runs the "Inference" stack. Students deploy their code here.
Sensors
RealSense Camera + Lidar
Connected to the Jetson to feed real-world data to the AI.
Actuator
Unitree Go2 or G1 (Shared)
Receives motor commands from the Jetson.


If you do not have access to RTX-enabled workstations, we must restructure the course to rely entirely on cloud-based instances (like AWS RoboMaker or NVIDIA's cloud delivery for Omniverse), though this introduces significant latency and cost complexity.

Building a "Physical AI" lab is a significant investment. You will have to choose between building a physical On-Premise Lab at Home (High CapEx) versus running a Cloud-Native Lab (High OpEx).

Option 2 High OpEx: The "Ether" Lab (Cloud-Native)
Best for: Rapid deployment, or students with weak laptops.
1. Cloud Workstations (AWS/Azure) Instead of buying PCs, you rent instances.
Instance Type: AWS g5.2xlarge (A10G GPU, 24GB VRAM) or g6e.xlarge.
Software: NVIDIA Isaac Sim on Omniverse Cloud (requires specific AMI).
Cost Calculation:
Instance cost: ~$1.50/hour (spot/on-demand mix).
Usage: 10 hours/week √ó 12 weeks = 120 hours.
Storage (EBS volumes for saving environments): ~$25/quarter.
Total Cloud Bill: ~$205 per quarter.
2. Local "Bridge" Hardware You cannot eliminate hardware entirely for "Physical AI." You still need the edge devices to deploy the code physically.
Edge AI Kits: You still need the Jetson Kit for the physical deployment phase.
Cost: $700 (One-time purchase).
Robot: You still need one physical robot for the final demo.
Cost: $3,000 (Unitree Go2 Standard).
The Economy Jetson Student Kit
Best for: Learning ROS 2, Basic Computer Vision, and Sim-to-Real control.
Component
Model
Price (Approx.)
Notes
The Brain
NVIDIA Jetson Orin Nano Super Dev Kit (8GB)
$249
New official MSRP (Price dropped from ~$499). Capable of 40 TOPS.
The Eyes
Intel RealSense D435i
$349
Includes IMU (essential for SLAM). Do not buy the D435 (non-i).
The Ears
ReSpeaker USB Mic Array v2.0
$69
Far-field microphone for voice commands (Module 4).
Wi-Fi
(Included in Dev Kit)
$0
The new "Super" kit includes the Wi-Fi module pre-installed.
Power/Misc
SD Card (128GB) + Jumper Wires
$30
High-endurance microSD card required for the OS.
TOTAL


~$700 per kit




3. The Latency Trap (Hidden Cost)
Simulating in the cloud works well, but controlling a real robot from a cloud instance is dangerous due to latency.
Solution: Students train in the Cloud, download the model (weights), and flash it to the local Jetson kit.

 
so please create a /sp.plan for my book .
here is a reference
/sp.plan

Create: architecture sketch, section structure, research approach, quality validation.
Decisions needing documentation: list important choices with options and tradeoffs.
Testing strategy: validation checks based on acceptance criteria.

Technical details:
- Use research-concurrent approach (research while writing, not all upfront)
- Follow APA citation style from Constitution
- Organize by phases: Research ‚Üí Foundation ‚Üí Analysis ‚Üí Synthesis
```

 # STEP 11.
 ## CHATGPT WILL CREATE PROMPT LIKE THIS COPY IT AND PASTE IT ON CLAUDE CODE.
 ```
 /sp.plan

Title: Physical AI & Humanoid Robotics Textbook ‚Äì Spec Plan

1. Architecture Sketch

1.1 Book Architecture (Docusaurus)
- Docusaurus site with MDX chapters.
- Chapter-level buttons:
  - Personalize Content
  - Translate to Urdu
  - Ask RAG Chatbot
- Pages deployed on GitHub Pages or Vercel.

1.2 Backend Services (FastAPI)
- RAG inference endpoint.
- Urdu translation endpoint.
- Personalization endpoint.
- Hosted on Vercel, Fly.io, or Render.

1.3 Data Layer
- Neon Serverless Postgres for user data.
- Qdrant Cloud for embeddings.
- File-based embedding pipeline for chapters.

1.4 Auth System
- better-auth.com for SignUp/SignIn.
- Capture user hardware and software background at signup.

1.5 RAG Chatbot
- OpenAI Agents / ChatKit Client SDK.
- Whole-book context and selection-based context.
- Uses Qdrant vectors and structured references.

1.6 Claude Code Subagents
- Research Agent
- ROS 2 Code Agent
- Simulation Agent
- Isaac/Perception Agent
- Book Formatting Agent
- All reusable and invoked from chapters as needed.

----------------------------------------------------

2. Section Structure (Full Book Outline)

Section 0 ‚Äî Introductory Material
- Preface
- Why Physical AI
- Course Overview
- How to Use This AI-Native Textbook
- Setup Checklist

Section 1 ‚Äî Foundations of Physical AI
- Chapter 1: Physical AI Concepts
- Chapter 2: Sensor Systems Overview
- Chapter 3: Embodied Intelligence
- Chapter 4: Humanoid Robotics Landscape

Section 2 ‚Äî ROS 2 Fundamentals
- Chapter 5: Nodes, Topics, Services, Actions
- Chapter 6: ROS 2 Python (rclpy)
- Chapter 7: URDF for Humanoids
- Chapter 8: Launch Files and Parameters
- Chapter 9: Connecting LLM Agents to ROS 2

Section 3 ‚Äî Digital Twin with Gazebo & Unity
- Chapter 10: Gazebo Overview
- Chapter 11: URDF and SDF Modeling
- Chapter 12: Physics Simulation
- Chapter 13: Sensor Simulation
- Chapter 14: Unity Visualization

Section 4 ‚Äî NVIDIA Isaac Platform
- Chapter 15: Isaac Sim Overview
- Chapter 16: Synthetic Data
- Chapter 17: Isaac ROS Perception
- Chapter 18: Navigation and VSLAM
- Chapter 19: Reinforcement Learning
- Chapter 20: Sim-to-Real Transfer

Section 5 ‚Äî Humanoid Development
- Chapter 21: Kinematics
- Chapter 22: Dynamics and Balance
- Chapter 23: Locomotion
- Chapter 24: Manipulation and Grasping
- Chapter 25: Human-Robot Interaction

Section 6 ‚Äî Vision-Language-Action
- Chapter 26: Whisper and Voice Commands
- Chapter 27: LLM to Action Planning
- Chapter 28: Multi-Modal Understanding
- Chapter 29: Cognitive Architectures
- Chapter 30: Behavior Trees and LLMs

Section 7 ‚Äî Capstone: Autonomous Humanoid
- Chapter 31: Architecture
- Chapter 32: Implementation Pipeline
- Chapter 33: Navigation and Object Retrieval
- Chapter 34: End-to-End Demo
- Chapter 35: Jetson Deployment

Section 8 ‚Äî Lab and Hardware
- Workstation Specs
- Jetson Setup
- RealSense Setup
- Robot Options

Appendices
- Full ROS Code
- Isaac Assets
- APA References
- Troubleshooting

----------------------------------------------------

3. Research Approach

Method: research-concurrent, not upfront.
Cycle per chapter:
1. Collect authoritative sources (ROS docs, Isaac docs, Gazebo docs, research papers).
2. Extract key concepts.
3. Draft outline.
4. Expand using verified sources.
5. Run Claude Code quality agents.
6. Format with APA.

Sources:
- ROS 2 Documentation
- Gazebo Documentation
- Isaac Sim/Isaac ROS Docs
- Jetson Documentation
- Academic Robotics Papers
- OpenAI Vision-Language-Action Papers
- Panaversity curriculum

Citation Style:
- APA style enforced.

----------------------------------------------------

4. Quality Validation Strategy

Textbook Acceptance Criteria:
- Technical accuracy validated against official docs.
- Beginner-to-advanced progression.
- All AI-native features implemented.
- ROS and Isaac code runs successfully.
- APA-compliant.
- Consistent terminology and structure.

Validation Agents:
- Research Validation Agent
- Factual Accuracy Agent
- Formatting Agent
- Pedagogy Agent
- Code QA Agent

----------------------------------------------------

5. Decisions Needing Documentation

5.1 Sidebar Structure
- Option A: Single large sidebar
- Option B: Section-based sidebars (chosen)

5.2 Embedding Model
- Option: OpenAI text-embedding-3-large (chosen)

5.3 Chunking Strategy
- Option: Semantic chunking (chosen)

5.4 Vector Database
- Qdrant (chosen over Pinecone)

5.5 Storage
- Neon Postgres for auth and personalization (chosen)

5.6 Personalization Logic
- Hybrid: Templates + LLM improvements (chosen)

5.7 Urdu Translation Logic
- Context-aware translation (chosen)

5.8 Claude Subagents
- Must declare: inputs, outputs, skills, reusability.

----------------------------------------------------

6. Testing Strategy

6.1 Book Testing
- Factual accuracy validation.
- APA citation checks.
- Pedagogy checks.
- ROS and Isaac code execution testing.

6.2 RAG Testing
- 20 known-answer tests.
- 10 tricky-question tests.
- 10 selection-based questions.
- Hallucination control.
- Paragraph reference validation.

6.3 Auth Testing
- Multiple user backgrounds tested.
- Personalization output differs correctly.
- Urdu translation accuracy validated.

6.4 Personalization Testing
Profiles:
- Beginner Python
- Intermediate Python
- ROS Beginner
- ROS Advanced
- No GPU
- Full Hardware Setup

6.5 Urdu Translation Testing
- Technical term preservation.
- Natural Urdu.
- Optional English labels in diagrams.

6.6 Performance Testing
- RAG latency < 1s
- Translation < 2s
- Personalization < 3s

----------------------------------------------------

7. Phase Breakdown

Phase 1: Research (Concurrent)
- Collect sources while writing.
- APA references inline.

Phase 2: Foundation
- Docusaurus setup.
- Auth system.
- Qdrant + Neon setup.
- Embedding pipeline.
- Base RAG.
- Claude subagents defined.

Phase 3: Analysis
- Deep ROS analysis.
- Deep Gazebo and Isaac analysis.
- Hardware architecture decisions.
- Document final architecture.

Phase 4: Synthesis
- Full chapter writing.
- Diagrams.
- Personalization and Urdu features.
- RAG completion.
- Final QA.
- Demo video creation.

End of File.

```

